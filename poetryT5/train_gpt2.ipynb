{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_gpt2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Convert CSV to TXT"
      ],
      "metadata": {
        "id": "ZzBxQsBXl01K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kciv1P2hwwBl"
      },
      "source": [
        "import csv\n",
        "csv_file = 'four-line-poetry.csv'\n",
        "txt_file = 'data.txt'\n",
        "with open(txt_file, \"w\") as my_output_file:\n",
        "    with open(csv_file, \"r\") as my_input_file:\n",
        "        for row in csv.reader(my_input_file):\n",
        "            my_output_file.write(row[2] + \":\" + row[1] + '\\n\\n')\n",
        "    my_output_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount and install packages"
      ],
      "metadata": {
        "id": "CBaZjF0gl4dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip3 install gpt-2-simple pronouncing g2p-en\n",
        "\n",
        "%cd drive/MyDrive/poetry/gpt2"
      ],
      "metadata": {
        "id": "kDsJUvobVuCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert samples from TXT to CSV"
      ],
      "metadata": {
        "id": "s5ls8fXyqXgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "scheme = \"abba\"\n",
        "prefix = scheme + \":\"\n",
        "file_name = \"samples/\" + scheme + \".txt\"\n",
        "csv_file_name = \"samples/\" + scheme + \".csv\"\n",
        "\n",
        "with open(file_name) as f:\n",
        "    with open(csv_file_name, \"w+\") as csv_file:\n",
        "        writer = csv.writer(\n",
        "            csv_file, delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL\n",
        "        )\n",
        "        writer.writerow([\"\", \"poem\", \"label\"])\n",
        "        lines = f.readlines()\n",
        "        counter = 0\n",
        "        for line in lines:\n",
        "            if line == \"\\n\" or len(line) < 15:\n",
        "                counter = 0\n",
        "                continue\n",
        "\n",
        "            if line.startswith(scheme):\n",
        "                counter = 1\n",
        "                line_1 = line.replace(scheme + \":\", \"\")\n",
        "                continue\n",
        "\n",
        "            if counter == 1:\n",
        "                counter = 2\n",
        "                line_2 = line\n",
        "                continue\n",
        "\n",
        "            if counter == 2:\n",
        "                counter = 3\n",
        "                line_3 = line\n",
        "                continue\n",
        "\n",
        "            if counter == 3:\n",
        "                counter = 0\n",
        "                line_4 = line.replace(\"\\n\", \"\")\n",
        "                writer.writerow([\"\", line_1 + line_2 + line_3 + line_4, scheme])\n",
        "                continue\n"
      ],
      "metadata": {
        "id": "V3eqriDWqXTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train GPT2"
      ],
      "metadata": {
        "id": "s6S1ndefl9R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import requests\n",
        "\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "file_name = \"data.txt\"\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              file_name,\n",
        "              model_name=model_name,\n",
        "              checkpoint_dir=\"checkpoint\",\n",
        "              batch_size=2,\n",
        "              accumulate_gradients=32,\n",
        "              learning_rate=0.001,\n",
        "              sample_every=50,\n",
        "              sample_length=200,\n",
        "              save_every=100,\n",
        "              steps=400)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess)"
      ],
      "metadata": {
        "id": "da6HuqHpwNJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate samples"
      ],
      "metadata": {
        "id": "a9gXWfbIl7ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess)\n",
        "\n",
        "gpt2.generate(sess, prefix=\"abba\",length=50,sample_delim=\"\\n\",nsamples=3000,destination_path=\"samples/abba.txt\")"
      ],
      "metadata": {
        "id": "CLYV3bDS7soe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc35de89-eeb9-4c53-972d-a8ea7f4c5d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-1900\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate minimum edit distance"
      ],
      "metadata": {
        "id": "3uuomiSiUIhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pronouncing\n",
        "import editdistance\n",
        "import nltk as nltk\n",
        "from nltk import word_tokenize\n",
        "from g2p_en import G2p\n",
        "from itertools import product\n",
        "from tqdm import tqdm\n",
        "\n",
        "# grapheme to phoneme converter\n",
        "g2p = G2p()\n",
        "# nltk.download('punkt')\n",
        "\n",
        "def evaluate(poetry, scheme):\n",
        "    \"\"\"\n",
        "    return the average minimum edit distance of the phonemes of the last words of each line of a four line poem\n",
        "    :param poetry: string / poem\n",
        "    :param scheme: string / rhyming scheme (aabb, abab, abba)\n",
        "    :return: float / average minimum edit distance of the phonemes of the last words\n",
        "    \"\"\"\n",
        "\n",
        "    def get_last_words(x):\n",
        "        \"\"\"\n",
        "        gets last alphabetical words from a list of sentences\n",
        "        :param x: list of strings / list of sentences\n",
        "        :return: list of strings / list of words\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return [[w for w in word_tokenize(l) if w.isalpha()][-1] for l in x]\n",
        "        except IndexError:\n",
        "            # if no last word can be found, return an empty string for that line\n",
        "            result = []\n",
        "            for l in x:\n",
        "                try:\n",
        "                    result.append([w for w in word_tokenize(l) if w.isalpha()][-1])\n",
        "                except IndexError:\n",
        "                    result.append('')\n",
        "            return result\n",
        "\n",
        "    def min_edit_distance(a, b, n=4):\n",
        "        \"\"\"\n",
        "        calculates minimum edit distance between word a and b based on their possible pronunciations\n",
        "        :param a: string / word\n",
        "        :param b: string / word\n",
        "        :param n: int / number of last phonemes to check, default 4\n",
        "        :return: float / minimum edit distance based on phonemes\n",
        "        \"\"\"\n",
        "        # get pronunciations\n",
        "        a_phonemes = pronouncing.phones_for_word(a)\n",
        "        if not a_phonemes:\n",
        "            a_phonemes = [' '.join(g2p(a))]\n",
        "        b_phonemes = pronouncing.phones_for_word(b)\n",
        "        if not b_phonemes:\n",
        "            b_phonemes = [' '.join(g2p(b))]\n",
        "\n",
        "        return min([editdistance.eval(c.split()[-n:], d.split()[-n:]) for c, d in product(a_phonemes, b_phonemes)],\n",
        "                   default=n)\n",
        "\n",
        "    last_words = get_last_words(poetry.split('\\n'))\n",
        "    if len(last_words) != 4:\n",
        "        if len(last_words) > 4:\n",
        "            last_words = last_words[:4]\n",
        "        else:\n",
        "            while len(last_words) < 4:\n",
        "                last_words.append('')\n",
        "\n",
        "    if scheme == 'abab':\n",
        "        return (min_edit_distance(last_words[0], last_words[2]) + min_edit_distance(last_words[1], last_words[3])) / 2\n",
        "    elif scheme == 'aabb':\n",
        "        return (min_edit_distance(last_words[0], last_words[1]) + min_edit_distance(last_words[2], last_words[3])) / 2\n",
        "    elif scheme == 'abba':\n",
        "        return (min_edit_distance(last_words[0], last_words[3]) + min_edit_distance(last_words[1], last_words[2])) / 2\n",
        "    else:\n",
        "        raise ValueError(scheme + ' is an invalid rhyming scheme. This code only works for the literals \\\"aabb\\\", '\n",
        "                                  '\\\"abab\\\" or \\\"abba\\\".')\n",
        "\n",
        "\n",
        "# Example on how to evaluate poems with given rhyming schemes\n",
        "df = pd.read_csv('samples/abba.csv', index_col=0)\n",
        "metric_vals = []\n",
        "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    metric_vals.append(evaluate(row.poem, row.label))\n",
        "print(\"\\n\")\n",
        "print(\"Average minimum edit distance per poem: \", np.average(metric_vals))"
      ],
      "metadata": {
        "id": "9TDhN4H3UKoe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}